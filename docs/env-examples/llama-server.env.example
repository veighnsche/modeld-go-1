# Example environment for running modeld against llama.cpp server
# Launch llama.cpp server separately, e.g.:
#   llama-server -m /path/to/model.gguf -c 4096 -ngl 0 -t 4 --host 127.0.0.1 --port 8081

# modeld flags can be passed via CLI; these envs are just examples for reference
MODELD_ADDR=:8080
MODELD_LOG_LEVEL=info

# Adapter configuration (pass as flags):
#   --llama-url, --llama-api-key, --llama-timeout, --llama-connect-timeout, --llama-use-openai
# Example values:
LLAMA_URL=http://127.0.0.1:8081
LLAMA_API_KEY=
LLAMA_TIMEOUT=30s
LLAMA_CONNECT_TIMEOUT=5s
LLAMA_USE_OPENAI=true

# Default model id exposed by llama-server (/v1/models)
DEFAULT_MODEL=your-model-id
