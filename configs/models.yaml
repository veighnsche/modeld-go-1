# Example configuration (YAML) for modeld
# Matches fields in internal/config/loader.go

# Server
addr: ":8080"
models_dir: "~/models/llm"

# VRAM budgeting (optional)
vram_budget_mb: 8192
vram_margin_mb: 512

# Default model to use when requests omit `model`
default_model: "llama-2-7b-q4"

# Observability / HTTP
log_level: "info"                # off|error|info|debug
max_body_bytes: 1048576           # 1 MiB
infer_timeout: "30s"             # cancel /infer after this duration

# CORS (optional)
cors_enabled: true
cors_allowed_origins:
  - "http://localhost:3000"
  - "http://127.0.0.1:3000"
cors_allowed_methods:
  - "GET"
  - "POST"
  - "OPTIONS"
cors_allowed_headers:
  - "Accept"
  - "Authorization"
  - "Content-Type"
  - "X-Requested-With"
  - "X-Log-Level"

# Backpressure controls (optional)
max_queue_depth: 16               # per-instance queue length cap
max_wait: "15s"                   # max time a request may wait in queue

# Real inference / llama.cpp (optional)
# Enable real inference (adapter-backed) rather than placeholder tokens
# real_infer: true
# Path to llama.cpp server binary (llama-server)
# llama_bin: "/home/USER/apps/llama.cpp/build/bin/llama-server"
# Context window size for llama.cpp
# llama_ctx: 4096
# Threads for llama.cpp (0=auto)
# llama_threads: 0
